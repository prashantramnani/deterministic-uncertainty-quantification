{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.utils.data\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "\n",
    "import ignite\n",
    "from ignite.engine import Events, Engine\n",
    "from ignite.metrics import Accuracy, Loss\n",
    "\n",
    "import numpy as np\n",
    "import sklearn.datasets\n",
    "from tqdm.notebook import trange, tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(1, 64, 3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(64, 128, 3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(128)\n",
    "\n",
    "        self.conv3 = nn.Conv2d(128, 128, 3)\n",
    "        self.bn3 = nn.BatchNorm2d(128)\n",
    "\n",
    "        self.fc1 = nn.Linear(2 * 2 * 128, 256)\n",
    "\n",
    "    def compute_features(self, x):\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "\n",
    "        x = F.relu(self.bn3(self.conv3(x)))\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "\n",
    "        x = x.flatten(1)\n",
    "\n",
    "        x = F.relu(self.fc1(x))\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class CNN_DUQ(Model):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_size,\n",
    "        num_classes,\n",
    "        embedding_size,\n",
    "        learnable_length_scale,\n",
    "        length_scale,\n",
    "        gamma,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.gamma = gamma\n",
    "\n",
    "        self.W = nn.Parameter(\n",
    "            torch.normal(torch.zeros(embedding_size, num_classes, 256), 0.05)\n",
    "        )\n",
    "\n",
    "        self.register_buffer(\"N\", torch.ones(num_classes) * 12)\n",
    "        self.register_buffer(\n",
    "            \"m\", torch.normal(torch.zeros(embedding_size, num_classes), 1)\n",
    "        )\n",
    "\n",
    "        self.m = self.m * self.N.unsqueeze(0)\n",
    "\n",
    "        if learnable_length_scale:\n",
    "            self.sigma = nn.Parameter(torch.zeros(num_classes) + length_scale)\n",
    "        else:\n",
    "            self.sigma = length_scale\n",
    "\n",
    "    def update_embeddings(self, x, y):\n",
    "        z = self.last_layer(self.compute_features(x))\n",
    "\n",
    "        # normalizing value per class, assumes y is one_hot encoded\n",
    "        self.N = self.gamma * self.N + (1 - self.gamma) * y.sum(0)\n",
    "\n",
    "        # compute sum of embeddings on class by class basis\n",
    "        features_sum = torch.einsum(\"ijk,ik->jk\", z, y)\n",
    "\n",
    "        self.m = self.gamma * self.m + (1 - self.gamma) * features_sum\n",
    "\n",
    "    def last_layer(self, z):\n",
    "        z = torch.einsum(\"ij,mnj->imn\", z, self.W)\n",
    "        return z\n",
    "\n",
    "    def output_layer(self, z):\n",
    "        embeddings = self.m / self.N.unsqueeze(0)\n",
    "\n",
    "        diff = z - embeddings.unsqueeze(0)\n",
    "        distances = (-(diff ** 2)).mean(1).div(2 * self.sigma ** 2).exp()\n",
    "\n",
    "        return distances\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.last_layer(self.compute_features(x))\n",
    "        y_pred = self.output_layer(z)\n",
    "\n",
    "        return z, y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 28\n",
    "num_classes = 10\n",
    "embedding_size = 256\n",
    "learnable_length_scale = False\n",
    "gamma = 0.999\n",
    "# length_scales = [0.05, 0.1, 0.2, 0.3, 0.5, 1.0]\n",
    "length_scale = 0.3\n",
    "\n",
    "model = CNN_DUQ(\n",
    "    input_size,\n",
    "    num_classes,\n",
    "    embedding_size,\n",
    "    learnable_length_scale,\n",
    "    length_scale,\n",
    "    gamma,\n",
    ")\n",
    "\n",
    "optimizer = torch.optim.SGD(\n",
    "    model.parameters(), lr=0.05, momentum=0.9, weight_decay=1e-4\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = torchvision.transforms.ToTensor()\n",
    "\n",
    "train_dataset = torchvision.datasets.MNIST(\"./\", train=True, download=True, transform=transform)\n",
    "test_dataset = torchvision.datasets.MNIST(\"./\", download=True, transform=transform)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=128, num_workers=4, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=128, num_workers=4, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPkAAAD3CAYAAADfRfLgAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAOh0lEQVR4nO3dX2hT9//H8VesSl1D1wsdCM5a55+h6RARdzGtbFAr2/wHdfMPLVtlcyJocbpqndPRYDvcdlOmY+KVbDBXb9T9EwXpsE6GrLWt6HbhhKqIMl2borbaz+/ixzcztU3aJE1O33s+oOA5pyfnZeorn8/JaTw+55wTALNGpDsAgKFFyQHjKDlgHCUHjKPkgHUuBSRFfDU3Nz+xzitfZCPbcM3VH18qLqH5fL6IZefcE+u8gmzxIdvgJTtXf1UeGc+D9fT0aPfu3bp8+bJGjx6tYDCo3NzchAICGBpxnZOfPHlSXV1d+vbbb/X++++rpqYm2bkAJElcI/n58+c1f/58SdKsWbPU0tIS9fubm5sVCAQi1qXgLCFuZIsP2QYvFbniKnkoFJLf7w8vZ2Rk6OHDhxo5su+Hy8/Pj1j26jmSRLZ4kW3wUnVOHtd03e/3q7OzM7zc09PTb8EBpFdcJZ89e7bq6+slSY2NjZo2bVpSQwFInriG38LCQp05c0YrV66Uc0579uxJdi4AScJ18l7IFh+yDZ6nz8kBDB+UHDCOkgPGUXLAOEoOGEfJAeMoOWAcJQeMo+SAcZQcMI6SA8ZRcsA4Sg4YR8kB4yg5YBwlB4yj5IBxlBwwjpIDxlFywDhKDhhHyQHjKDlgHCUHjKPkgHGUHDCOkgPGUXLAOEoOGBfXrYsx/I0ZMybq9mPHjkXd/sorrzyxrqenJ/znhQsX9rvvyZMnY6RDMsVd8uXLl8vv90uSJkyYoOrq6qSFApA8cZX8wYMHcs7p0KFDyc4DIMniOie/dOmS7t27p7KyMpWWlqqxsTHZuQAkic855wa70+XLl9XU1KQVK1bor7/+0jvvvKOffvpJI0f2PTFoaWlRIBBIOCyAwYtrup6Xl6fc3Fz5fD7l5eUpJydHt27d0vjx4/v8/vz8/Ihl55x8Pl88hx5y/5VsyX7jzefz6fHxwktvvHn1Z5rsXP2N13FN1+vq6lRTUyNJunnzpkKhkMaNGxd/OgBDJq6RvLi4WNu3b9eqVavk8/m0Z8+efqfqANIrrmaOHj1an332WbKzIIX6us49mO29p4a9p+vM7LyD33gDjKPkgHGUHDCOkgPGUXLAOEoOGEfJAeMoOWAcJQeMo+SAcZQcMI6SA8ZRcsA4Sg4YR8kB4yg5YBwlB4yj5IBxlBwwjpIDxlFywDhKDhhHyQHjKDlgHCUHjKPkgHGUHDCOkgPGUXLAOEoOGMdNxdEnn88XdfuDBw8iljMzM9XV1RVePn78+JDkwuANaCRvampSSUmJJOnq1atatWqVVq9erV27dqmnp2dIAwJITMySHzhwQB9++GH4lbu6ulrl5eX65ptv5JzTqVOnhjwkgPjFLPnEiRNVW1sbXm5tbdXcuXMlSQUFBWpoaBi6dAASFvOcvKioSG1tbeFl51z4fC0rK0sdHR0xD9Lc3KxAIBCxzjk32KwpQ7bYMjMzo65rb29PZZyYvPK89ZaKXIN+423EiH8H/87OTmVnZ8fcJz8/P2L58RcKr/mvZHvttdeibo/1xtn9+/cjljMzMyPWPfPMM/3uO5CBIZm8+jNNdq7+XjAGfQltxowZOnfunCSpvr5ec+bMSSwZgCE16JJXVFSotrZWb775prq7u1VUVDQUuQAkic+l4KSg95TEq9Mn6b+TLdZ0/dixY1G3//LLLxHLBQUFqq+vDy8vWLAg/nBJ5tWfqWen6wCGF0oOGEfJAeMoOWAcJQeMo+SAcXzU9D9q4cKFCe1//fr1Aa1D+jGSA8ZRcsA4Sg4YR8kB4yg5YBwlB4yj5IBxXCf/jxo5MrEffWNjY8TyypUrn1gHb2AkB4yj5IBxlBwwjpIDxlFywDhKDhhHyQHjKDlgHCUHjKPkgHGUHDCOkgPGUXLAOEoOGEfJAeP4PLlRkyZNirr9rbfeiro91i11+9ruxdsDY4AjeVNTk0pKSiRJFy9e1Pz581VSUqKSkhL98MMPQxoQQGJijuQHDhzQ0aNHNWbMGElSa2ur3n77bZWVlQ15OACJizmST5w4UbW1teHllpYWnT59WmvWrFFlZaVCodCQBgSQGJ9zzsX6pra2Nm3evFmHDx/WkSNHNH36dAUCAe3fv1/t7e2qqKiIun9LS4sCgUDSQgMYuEG/8VZYWKjs7Ozwn6uqqmLuk5+fH7HsnPPsmzRWssV64621tTXq9qeeeirq9u3bt0csV1dXR6yrqamJHjCFvPozTXau/sbrQV9CW7t2rS5cuCBJOnv2rGbOnJlYMgBDatAj+e7du1VVVaVRo0Zp7NixAxrJAaTPgM7JEz5IrymJV6dPkp1ssd4DaWpqiro91nHmzZsXsXzmzBm99NJL4eWGhoYYCVPHqz9Tz07XAQwvlBwwjpIDxlFywDhKDhhHyQHj+KipURMmTEho/2PHjkXdfu7cuQGtQ/oxkgPGUXLAOEoOGEfJAeMoOWAcJQeMo+SAcVwnN2rWrFkJ7d/W1hZ1+6NHjwa0DunHSA4YR8kB4yg5YBwlB4yj5IBxlBwwjpIDxnGd3KiXX345of1v3bqVpCRIN0ZywDhKDhhHyQHjKDlgHCUHjKPkgHGUHDCO6+TDWE5OTr/bCgsLE3rs3377LaH94R1RS97d3a3Kykpdu3ZNXV1dWr9+vaZMmaJt27bJ5/Np6tSp2rVrl0aMYEIAeFXUkh89elQ5OTnau3ev7t69q2XLlun5559XeXm5XnzxRX300Uc6depUwqMGgKETdQhetGiRNm3aJElyzikjI0Otra2aO3euJKmgoEANDQ1DnxJA3KKO5FlZWZKkUCikjRs3qry8XJ988ol8Pl94e0dHR8yDNDc3KxAIRKxzzsWbeciRTTp+/Pig9+F5G7xU5Ir5xtuNGze0YcMGrV69WosXL9bevXvD2zo7O5WdnR3zIPn5+RHLzrnwC4XXDKds0d54+/vvvxM61uLFi6Nu//7776Nm8xKvZkt2rv5eMKJO12/fvq2ysjJt3bpVxcXFkqQZM2aE715ZX1+vOXPmJC0kgOSLOpJ/+eWXam9v1759+7Rv3z5J0o4dOxQMBvX5559r8uTJKioqSklQPCnaVM+r01Okns+l4F9D7ymJV6dP0vDK9vTTT/f7vYlO15csWRJ1O9P1xHliug5g+KPkgHGUHDCOkgPGUXLAOEoOGMdHTYexV199dcge+8SJE0P22EgtRnLAOEoOGEfJAeMoOWAcJQeMo+SAcZQcMI7r5MNYQUFB3PvG+m+7uru7435seAsjOWAcJQeMo+SAcZQcMI6SA8ZRcsA4Sg4Yx3XyYWz//v39bnv33Xej7ltXV5fsOPAoRnLAOEoOGEfJAeMoOWAcJQeMo+SAcZQcMI5bF/dCtviQbfBSdeviqL8M093drcrKSl27dk1dXV1av369xo8fr3Xr1mnSpEmSpFWrVg3pf/IPIDFRR/IjR47o0qVL2rFjh+7evatly5Zpw4YN6ujoUFlZ2cAPwkieFGSLj1ezpWokj1ryzs5OOefk9/t1584dFRcXa968ebpy5YoePXqk3NxcVVZWyu/3Rz14S0uLAoFAYn8DAHEZ0Dl5KBTS+vXr9cYbb6irq0vTp09XIBDQ/v371d7eroqKiugHYSRPCrLFx6vZUjWSy8Vw/fp1t3z5cvfdd98555z7559/wtv+/PNPV1paGushnKSIr77WeeWLbGQbrrn6E/US2u3bt1VWVqatW7equLhYkrR27VpduHBBknT27FnNnDkz2kMASLOo0/VgMKgff/xRkydPDq8rLy/X3r17NWrUKI0dO1ZVVVUxz8mZricH2eLj1Wypmq5znbwXssWHbIOXqpLzG2+AcZQcMI6SA8ZRcsA4Sg4YR8kB4yg5YBwlB4yj5IBxlBwwjpIDxlFywDhKDhhHyQHjUvJRUwDpw0gOGEfJAeMoOWAcJQeMo+SAcZQcMI6SA8ZFvatpsvX09Gj37t26fPmyRo8erWAwqNzc3FRGiGr58uXh/0N+woQJqq6uTmuepqYmffrppzp06JCuXr2qbdu2yefzaerUqdq1a5dGjEjfa/Tj2S5evOiJO932dRfeKVOmeOJ5S+sdgmPe4yiJfv75Z1dRUeGcc+7333937733XioPH9X9+/fd0qVL0x0j7KuvvnKvv/66W7FihXPOuXXr1rlff/3VOefczp073YkTJzyT7fDhw+7gwYNpy/M/dXV1LhgMOuecu3PnjluwYIFnnre+sqXqeUvpS9r58+c1f/58SdKsWbPU0tKSysNHdenSJd27d09lZWUqLS1VY2NjWvNMnDhRtbW14eXW1lbNnTtXklRQUKCGhoZ0RXsiW0tLi06fPq01a9aosrJSoVAoLbkWLVqkTZs2Sfr/Gw1kZGR45nnrK1uqnreUljwUCkXcUikjI0MPHz5MZYR+ZWZmau3atTp48KA+/vhjbdmyJa3ZioqKNHLkv2dT7rG7bWRlZamjoyNd0Z7I9sILL+iDDz7Q119/rWeffVZffPFFWnJlZWXJ7/crFApp48aNKi8v98zz1le2VD1vKS253+9XZ2dneLmnpyfiH0s65eXlacmSJfL5fMrLy1NOTo5u3bqV7lhhj59HdnZ2Kjs7O41pIhUWFobvP19YWKiLFy+mLcuNGzdUWlqqpUuXavHixZ563npnS9XzltKSz549W/X19ZKkxsZGTZs2LZWHj6qurk41NTWSpJs3byoUCmncuHFpTvWvGTNm6Ny5c5Kk+vp6zZkzJ82J/uWVO932dRderzxv6bxDcEo/hfa/d9f/+OMPOee0Z88ePffcc6k6fFRdXV3avn27rl+/Lp/Ppy1btmj27NlpzdTW1qbNmzfr8OHDunLlinbu3Knu7m5NnjxZwWBQGRkZnsjW2tqqqqqqQd3pdij0dRfeHTt2KBgMpv15S9YdguPBR00B4/hlGMA4Sg4YR8kB4yg5YBwlB4yj5IBxlBww7v8ABsOvJUrXKWoAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataloader_iterator = iter(train_loader)\n",
    "x, y = next(dataloader_iterator)\n",
    "y = F.one_hot(y, num_classes=10).float()\n",
    "y[0]\n",
    "img = x[2].view(28, 28).data\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(img, cmap='gray')\n",
    "plt.show()   \n",
    "# type(x[0].shape)\n",
    "# print(x[0])\n",
    "a = model(x)\n",
    "# a ** 2\n",
    "# x torch.Size([128, 1, 28, 28])\n",
    "# z torch.Size([128, 256, 10])\n",
    "# DIFF torch.Size([128, 256, 10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LOSS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_gradient_penalty(x, y_pred_sum):\n",
    "    gradients = torch.autograd.grad(\n",
    "        outputs=y_pred_sum,\n",
    "        inputs=x,\n",
    "        grad_outputs=torch.ones_like(y_pred_sum),\n",
    "        create_graph=True,\n",
    "        retain_graph=True,\n",
    "    )[0]\n",
    "    gradients = gradients.flatten(start_dim=1)\n",
    "\n",
    "    # L2 norm\n",
    "    grad_norm = gradients.norm(2, dim=1)\n",
    "\n",
    "    # Two sided penalty\n",
    "    gradient_penalty = ((grad_norm - 1) ** 2).mean()\n",
    "\n",
    "    return gradient_penalty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Traning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration : 0/469 LOSS : 0.653183102607727\n",
      "iteration : 1/469 LOSS : 0.6336632966995239\n",
      "iteration : 2/469 LOSS : 0.6166045069694519\n",
      "iteration : 3/469 LOSS : 0.5872936248779297\n",
      "iteration : 4/469 LOSS : 0.5796610116958618\n",
      "iteration : 5/469 LOSS : 0.5668841004371643\n",
      "iteration : 6/469 LOSS : 0.5619021654129028\n",
      "iteration : 7/469 LOSS : 0.5555239319801331\n",
      "iteration : 8/469 LOSS : 0.5519610047340393\n",
      "iteration : 9/469 LOSS : 0.5490432381629944\n",
      "iteration : 10/469 LOSS : 0.5475823879241943\n",
      "iteration : 11/469 LOSS : 0.5326354503631592\n",
      "iteration : 12/469 LOSS : 0.5250062942504883\n",
      "iteration : 13/469 LOSS : 0.5165350437164307\n",
      "iteration : 14/469 LOSS : 0.5144238471984863\n",
      "iteration : 15/469 LOSS : 0.5043741464614868\n",
      "iteration : 16/469 LOSS : 0.49769675731658936\n",
      "iteration : 17/469 LOSS : 0.4914644658565521\n",
      "iteration : 18/469 LOSS : 0.4800145924091339\n",
      "iteration : 19/469 LOSS : 0.4754343628883362\n",
      "iteration : 20/469 LOSS : 0.46802374720573425\n",
      "iteration : 21/469 LOSS : 0.46246445178985596\n",
      "iteration : 22/469 LOSS : 0.44740554690361023\n",
      "iteration : 23/469 LOSS : 0.43278610706329346\n",
      "iteration : 24/469 LOSS : 0.4204481244087219\n",
      "iteration : 25/469 LOSS : 0.41795456409454346\n",
      "iteration : 26/469 LOSS : 0.39886659383773804\n",
      "iteration : 27/469 LOSS : 0.395194411277771\n",
      "iteration : 28/469 LOSS : 0.3849753141403198\n",
      "iteration : 29/469 LOSS : 0.3725198805332184\n",
      "iteration : 30/469 LOSS : 0.35473188757896423\n",
      "iteration : 31/469 LOSS : 0.34843772649765015\n",
      "iteration : 32/469 LOSS : 0.3363620340824127\n",
      "iteration : 33/469 LOSS : 0.3277525007724762\n",
      "iteration : 34/469 LOSS : 0.3131609261035919\n",
      "iteration : 35/469 LOSS : 0.30389443039894104\n",
      "iteration : 36/469 LOSS : 0.29538586735725403\n",
      "iteration : 37/469 LOSS : 0.2812946140766144\n",
      "iteration : 38/469 LOSS : 0.27011746168136597\n",
      "iteration : 39/469 LOSS : 0.25874510407447815\n",
      "iteration : 40/469 LOSS : 0.2476751208305359\n",
      "iteration : 41/469 LOSS : 0.25067344307899475\n",
      "iteration : 42/469 LOSS : 0.25116631388664246\n",
      "iteration : 43/469 LOSS : 0.2400408536195755\n",
      "iteration : 44/469 LOSS : 0.23598738014698029\n",
      "iteration : 45/469 LOSS : 0.2238679975271225\n",
      "iteration : 46/469 LOSS : 0.21554532647132874\n",
      "iteration : 47/469 LOSS : 0.20822742581367493\n",
      "iteration : 48/469 LOSS : 0.21113340556621552\n",
      "iteration : 49/469 LOSS : 0.20431208610534668\n",
      "iteration : 50/469 LOSS : 0.2004680335521698\n",
      "iteration : 51/469 LOSS : 0.18837310373783112\n",
      "iteration : 52/469 LOSS : 0.18564139306545258\n",
      "iteration : 53/469 LOSS : 0.18569979071617126\n",
      "iteration : 54/469 LOSS : 0.17856603860855103\n",
      "iteration : 55/469 LOSS : 0.17721328139305115\n",
      "iteration : 56/469 LOSS : 0.17232929170131683\n",
      "iteration : 57/469 LOSS : 0.17170989513397217\n",
      "iteration : 58/469 LOSS : 0.1631312221288681\n",
      "iteration : 59/469 LOSS : 0.1500733345746994\n",
      "iteration : 60/469 LOSS : 0.16012264788150787\n",
      "iteration : 61/469 LOSS : 0.15481878817081451\n",
      "iteration : 62/469 LOSS : 0.14632782340049744\n",
      "iteration : 63/469 LOSS : 0.14907923340797424\n",
      "iteration : 64/469 LOSS : 0.13901300728321075\n",
      "iteration : 65/469 LOSS : 0.1335858404636383\n",
      "iteration : 66/469 LOSS : 0.13824677467346191\n",
      "iteration : 67/469 LOSS : 0.1302400678396225\n",
      "iteration : 68/469 LOSS : 0.12680703401565552\n",
      "iteration : 69/469 LOSS : 0.12709417939186096\n",
      "iteration : 70/469 LOSS : 0.12520825862884521\n",
      "iteration : 71/469 LOSS : 0.13240033388137817\n",
      "iteration : 72/469 LOSS : 0.12267384678125381\n",
      "iteration : 73/469 LOSS : 0.13320867717266083\n",
      "iteration : 74/469 LOSS : 0.12604200839996338\n",
      "iteration : 75/469 LOSS : 0.11691977083683014\n",
      "iteration : 76/469 LOSS : 0.12781034409999847\n",
      "iteration : 77/469 LOSS : 0.1071961298584938\n",
      "iteration : 78/469 LOSS : 0.11576098203659058\n",
      "iteration : 79/469 LOSS : 0.10652795433998108\n",
      "iteration : 80/469 LOSS : 0.10731881856918335\n",
      "iteration : 81/469 LOSS : 0.11240244656801224\n",
      "iteration : 82/469 LOSS : 0.09595989435911179\n",
      "iteration : 83/469 LOSS : 0.09714419394731522\n",
      "iteration : 84/469 LOSS : 0.09822190552949905\n",
      "iteration : 85/469 LOSS : 0.10476618260145187\n",
      "iteration : 86/469 LOSS : 0.10479260981082916\n",
      "iteration : 87/469 LOSS : 0.09781742095947266\n",
      "iteration : 88/469 LOSS : 0.0833970233798027\n",
      "iteration : 89/469 LOSS : 0.08508024364709854\n",
      "iteration : 90/469 LOSS : 0.0916525274515152\n",
      "iteration : 91/469 LOSS : 0.08874648809432983\n",
      "iteration : 92/469 LOSS : 0.07704848051071167\n",
      "iteration : 93/469 LOSS : 0.09667947143316269\n",
      "iteration : 94/469 LOSS : 0.07892227917909622\n",
      "iteration : 95/469 LOSS : 0.08669863641262054\n",
      "iteration : 96/469 LOSS : 0.08810798823833466\n",
      "iteration : 97/469 LOSS : 0.08401000499725342\n",
      "iteration : 98/469 LOSS : 0.07996600866317749\n",
      "iteration : 99/469 LOSS : 0.09639193117618561\n",
      "iteration : 100/469 LOSS : 0.07702815532684326\n",
      "iteration : 101/469 LOSS : 0.07759902626276016\n",
      "iteration : 102/469 LOSS : 0.07460536807775497\n",
      "iteration : 103/469 LOSS : 0.0690128281712532\n",
      "iteration : 104/469 LOSS : 0.0819779708981514\n",
      "iteration : 105/469 LOSS : 0.07969219237565994\n",
      "iteration : 106/469 LOSS : 0.07141944766044617\n",
      "iteration : 107/469 LOSS : 0.06722567975521088\n",
      "iteration : 108/469 LOSS : 0.07914514094591141\n",
      "iteration : 109/469 LOSS : 0.05282289534807205\n",
      "iteration : 110/469 LOSS : 0.07336568832397461\n",
      "iteration : 111/469 LOSS : 0.07091966271400452\n",
      "iteration : 112/469 LOSS : 0.07751849293708801\n",
      "iteration : 113/469 LOSS : 0.07418230175971985\n",
      "iteration : 114/469 LOSS : 0.06197497621178627\n",
      "iteration : 115/469 LOSS : 0.06534508615732193\n",
      "iteration : 116/469 LOSS : 0.06189245358109474\n",
      "iteration : 117/469 LOSS : 0.06093132495880127\n",
      "iteration : 118/469 LOSS : 0.0613226555287838\n",
      "iteration : 119/469 LOSS : 0.06207038089632988\n",
      "iteration : 120/469 LOSS : 0.06978115439414978\n",
      "iteration : 121/469 LOSS : 0.06828197836875916\n",
      "iteration : 122/469 LOSS : 0.06525246798992157\n",
      "iteration : 123/469 LOSS : 0.06492289900779724\n",
      "iteration : 124/469 LOSS : 0.06760434061288834\n",
      "iteration : 125/469 LOSS : 0.06012316793203354\n",
      "iteration : 126/469 LOSS : 0.05818147212266922\n",
      "iteration : 127/469 LOSS : 0.058540359139442444\n",
      "iteration : 128/469 LOSS : 0.06252332031726837\n",
      "iteration : 129/469 LOSS : 0.06358934193849564\n",
      "iteration : 130/469 LOSS : 0.058236829936504364\n",
      "iteration : 131/469 LOSS : 0.061746422201395035\n",
      "iteration : 132/469 LOSS : 0.06477954983711243\n",
      "iteration : 133/469 LOSS : 0.0610792450606823\n",
      "iteration : 134/469 LOSS : 0.06340157985687256\n",
      "iteration : 135/469 LOSS : 0.062581367790699\n",
      "iteration : 136/469 LOSS : 0.06617362797260284\n",
      "iteration : 137/469 LOSS : 0.05866824463009834\n",
      "iteration : 138/469 LOSS : 0.07160869240760803\n",
      "iteration : 139/469 LOSS : 0.05525398254394531\n",
      "iteration : 140/469 LOSS : 0.065301313996315\n",
      "iteration : 141/469 LOSS : 0.054772183299064636\n",
      "iteration : 142/469 LOSS : 0.06231909990310669\n",
      "iteration : 143/469 LOSS : 0.05914299935102463\n",
      "iteration : 144/469 LOSS : 0.06159505993127823\n",
      "iteration : 145/469 LOSS : 0.06419680267572403\n",
      "iteration : 146/469 LOSS : 0.054068125784397125\n",
      "iteration : 147/469 LOSS : 0.059443965554237366\n",
      "iteration : 148/469 LOSS : 0.05866577476263046\n",
      "iteration : 149/469 LOSS : 0.05358448624610901\n",
      "iteration : 150/469 LOSS : 0.062371473759412766\n",
      "iteration : 151/469 LOSS : 0.0642414391040802\n",
      "iteration : 152/469 LOSS : 0.05772125720977783\n",
      "iteration : 153/469 LOSS : 0.05508267506957054\n",
      "iteration : 154/469 LOSS : 0.05945424735546112\n",
      "iteration : 155/469 LOSS : 0.05467560887336731\n",
      "iteration : 156/469 LOSS : 0.055412378162145615\n",
      "iteration : 157/469 LOSS : 0.05454806983470917\n",
      "iteration : 158/469 LOSS : 0.05773676931858063\n",
      "iteration : 159/469 LOSS : 0.057573623955249786\n",
      "iteration : 160/469 LOSS : 0.049366582185029984\n",
      "iteration : 161/469 LOSS : 0.05518975108861923\n",
      "iteration : 162/469 LOSS : 0.04670094698667526\n",
      "iteration : 163/469 LOSS : 0.05845656991004944\n",
      "iteration : 164/469 LOSS : 0.05455689877271652\n",
      "iteration : 165/469 LOSS : 0.05743194743990898\n",
      "iteration : 166/469 LOSS : 0.049437008798122406\n",
      "iteration : 167/469 LOSS : 0.05678041651844978\n",
      "iteration : 168/469 LOSS : 0.04849463701248169\n",
      "iteration : 169/469 LOSS : 0.05409346520900726\n",
      "iteration : 170/469 LOSS : 0.04836751148104668\n",
      "iteration : 171/469 LOSS : 0.05250687524676323\n",
      "iteration : 172/469 LOSS : 0.0529235303401947\n",
      "iteration : 173/469 LOSS : 0.05861895531415939\n",
      "iteration : 174/469 LOSS : 0.052475281059741974\n",
      "iteration : 175/469 LOSS : 0.05600591003894806\n",
      "iteration : 176/469 LOSS : 0.05182364583015442\n",
      "iteration : 177/469 LOSS : 0.04313155263662338\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration : 178/469 LOSS : 0.05641383305191994\n",
      "iteration : 179/469 LOSS : 0.04361452907323837\n",
      "iteration : 180/469 LOSS : 0.04904372990131378\n",
      "iteration : 181/469 LOSS : 0.05626717582345009\n",
      "iteration : 182/469 LOSS : 0.0591466948390007\n",
      "iteration : 183/469 LOSS : 0.04511416703462601\n",
      "iteration : 184/469 LOSS : 0.049936436116695404\n",
      "iteration : 185/469 LOSS : 0.04882135987281799\n",
      "iteration : 186/469 LOSS : 0.04337616264820099\n",
      "iteration : 187/469 LOSS : 0.05072791129350662\n",
      "iteration : 188/469 LOSS : 0.0536622628569603\n",
      "iteration : 189/469 LOSS : 0.05393724516034126\n",
      "iteration : 190/469 LOSS : 0.05605033412575722\n",
      "iteration : 191/469 LOSS : 0.04877360537648201\n",
      "iteration : 192/469 LOSS : 0.05368121340870857\n",
      "iteration : 193/469 LOSS : 0.052041273564100266\n",
      "iteration : 194/469 LOSS : 0.05272790417075157\n",
      "iteration : 195/469 LOSS : 0.04915449395775795\n",
      "iteration : 196/469 LOSS : 0.05273262411355972\n",
      "iteration : 197/469 LOSS : 0.05925128981471062\n",
      "iteration : 198/469 LOSS : 0.04412857070565224\n",
      "iteration : 199/469 LOSS : 0.039127416908741\n",
      "iteration : 200/469 LOSS : 0.05120358243584633\n",
      "iteration : 201/469 LOSS : 0.05433912202715874\n",
      "iteration : 202/469 LOSS : 0.047868791967630386\n",
      "iteration : 203/469 LOSS : 0.04345018044114113\n",
      "iteration : 204/469 LOSS : 0.0420173741877079\n",
      "iteration : 205/469 LOSS : 0.051617734134197235\n",
      "iteration : 206/469 LOSS : 0.043198924511671066\n",
      "iteration : 207/469 LOSS : 0.046885352581739426\n",
      "iteration : 208/469 LOSS : 0.048378877341747284\n",
      "iteration : 209/469 LOSS : 0.0512576662003994\n",
      "iteration : 210/469 LOSS : 0.042049724608659744\n",
      "iteration : 211/469 LOSS : 0.045749012380838394\n",
      "iteration : 212/469 LOSS : 0.04484616219997406\n",
      "iteration : 213/469 LOSS : 0.05746131390333176\n",
      "iteration : 214/469 LOSS : 0.050533875823020935\n",
      "iteration : 215/469 LOSS : 0.049498431384563446\n",
      "iteration : 216/469 LOSS : 0.04797583073377609\n",
      "iteration : 217/469 LOSS : 0.04433197155594826\n",
      "iteration : 218/469 LOSS : 0.054202400147914886\n",
      "iteration : 219/469 LOSS : 0.054600734263658524\n",
      "iteration : 220/469 LOSS : 0.04971903935074806\n",
      "iteration : 221/469 LOSS : 0.045556798577308655\n",
      "iteration : 222/469 LOSS : 0.04320331662893295\n",
      "iteration : 223/469 LOSS : 0.05206604301929474\n",
      "iteration : 224/469 LOSS : 0.04493369534611702\n",
      "iteration : 225/469 LOSS : 0.04015421122312546\n",
      "iteration : 226/469 LOSS : 0.04327356815338135\n",
      "iteration : 227/469 LOSS : 0.044092923402786255\n",
      "iteration : 228/469 LOSS : 0.0435364730656147\n",
      "iteration : 229/469 LOSS : 0.04322243854403496\n",
      "iteration : 230/469 LOSS : 0.045112498104572296\n",
      "iteration : 231/469 LOSS : 0.04159288853406906\n",
      "iteration : 232/469 LOSS : 0.045036010444164276\n",
      "iteration : 233/469 LOSS : 0.051244817674160004\n",
      "iteration : 234/469 LOSS : 0.04173605889081955\n",
      "iteration : 235/469 LOSS : 0.040129147469997406\n",
      "iteration : 236/469 LOSS : 0.04431651532649994\n",
      "iteration : 237/469 LOSS : 0.0467052161693573\n",
      "iteration : 238/469 LOSS : 0.0508105531334877\n",
      "iteration : 239/469 LOSS : 0.0406741201877594\n",
      "iteration : 240/469 LOSS : 0.04688882455229759\n",
      "iteration : 241/469 LOSS : 0.04416521638631821\n",
      "iteration : 242/469 LOSS : 0.044661298394203186\n",
      "iteration : 243/469 LOSS : 0.05348851904273033\n",
      "iteration : 244/469 LOSS : 0.05614321678876877\n",
      "iteration : 245/469 LOSS : 0.03859382122755051\n",
      "iteration : 246/469 LOSS : 0.03960281237959862\n",
      "iteration : 247/469 LOSS : 0.04336383193731308\n",
      "iteration : 248/469 LOSS : 0.040174953639507294\n",
      "iteration : 249/469 LOSS : 0.03771908953785896\n",
      "iteration : 250/469 LOSS : 0.03704485669732094\n",
      "iteration : 251/469 LOSS : 0.043060436844825745\n",
      "iteration : 252/469 LOSS : 0.039709076285362244\n",
      "iteration : 253/469 LOSS : 0.04226740449666977\n",
      "iteration : 254/469 LOSS : 0.04204131290316582\n",
      "iteration : 255/469 LOSS : 0.03913378715515137\n",
      "iteration : 256/469 LOSS : 0.04489206150174141\n",
      "iteration : 257/469 LOSS : 0.04171541705727577\n",
      "iteration : 258/469 LOSS : 0.04413650184869766\n",
      "iteration : 259/469 LOSS : 0.040261201560497284\n",
      "iteration : 260/469 LOSS : 0.038222458213567734\n",
      "iteration : 261/469 LOSS : 0.043763335794210434\n",
      "iteration : 262/469 LOSS : 0.04444583132863045\n",
      "iteration : 263/469 LOSS : 0.04288552701473236\n",
      "iteration : 264/469 LOSS : 0.04974086582660675\n",
      "iteration : 265/469 LOSS : 0.0367288663983345\n",
      "iteration : 266/469 LOSS : 0.04386129602789879\n",
      "iteration : 267/469 LOSS : 0.0487825982272625\n",
      "iteration : 268/469 LOSS : 0.04751698672771454\n",
      "iteration : 269/469 LOSS : 0.04045189544558525\n",
      "iteration : 270/469 LOSS : 0.04368356987833977\n",
      "iteration : 271/469 LOSS : 0.04145867004990578\n",
      "iteration : 272/469 LOSS : 0.04547389596700668\n",
      "iteration : 273/469 LOSS : 0.04334057494997978\n",
      "iteration : 274/469 LOSS : 0.04365955665707588\n",
      "iteration : 275/469 LOSS : 0.04592215269804001\n",
      "iteration : 276/469 LOSS : 0.044117655605077744\n",
      "iteration : 277/469 LOSS : 0.03993483632802963\n",
      "iteration : 278/469 LOSS : 0.03996538743376732\n",
      "iteration : 279/469 LOSS : 0.042033325880765915\n",
      "iteration : 280/469 LOSS : 0.04045053571462631\n",
      "iteration : 281/469 LOSS : 0.03829377889633179\n",
      "iteration : 282/469 LOSS : 0.04353535547852516\n",
      "iteration : 283/469 LOSS : 0.0363469272851944\n",
      "iteration : 284/469 LOSS : 0.03862154483795166\n",
      "iteration : 285/469 LOSS : 0.03840143233537674\n",
      "iteration : 286/469 LOSS : 0.044829439371824265\n",
      "iteration : 287/469 LOSS : 0.03656819462776184\n",
      "iteration : 288/469 LOSS : 0.04422814026474953\n",
      "iteration : 289/469 LOSS : 0.043457284569740295\n",
      "iteration : 290/469 LOSS : 0.038206879049539566\n",
      "iteration : 291/469 LOSS : 0.04133414477109909\n",
      "iteration : 292/469 LOSS : 0.03990241885185242\n",
      "iteration : 293/469 LOSS : 0.041334476321935654\n",
      "iteration : 294/469 LOSS : 0.04792942851781845\n",
      "iteration : 295/469 LOSS : 0.03772049397230148\n",
      "iteration : 296/469 LOSS : 0.04122663661837578\n",
      "iteration : 297/469 LOSS : 0.045377008616924286\n",
      "iteration : 298/469 LOSS : 0.041323013603687286\n",
      "iteration : 299/469 LOSS : 0.0571286641061306\n",
      "iteration : 300/469 LOSS : 0.042321622371673584\n",
      "iteration : 301/469 LOSS : 0.03966406360268593\n",
      "iteration : 302/469 LOSS : 0.04222925379872322\n",
      "iteration : 303/469 LOSS : 0.040611475706100464\n",
      "iteration : 304/469 LOSS : 0.03475790470838547\n",
      "iteration : 305/469 LOSS : 0.042457252740859985\n",
      "iteration : 306/469 LOSS : 0.03750811889767647\n",
      "iteration : 307/469 LOSS : 0.040394071489572525\n",
      "iteration : 308/469 LOSS : 0.03867381438612938\n",
      "iteration : 309/469 LOSS : 0.03884381428360939\n",
      "iteration : 310/469 LOSS : 0.04199833795428276\n",
      "iteration : 311/469 LOSS : 0.04798251762986183\n",
      "iteration : 312/469 LOSS : 0.04751628637313843\n",
      "iteration : 313/469 LOSS : 0.05165794864296913\n",
      "iteration : 314/469 LOSS : 0.03515743836760521\n",
      "iteration : 315/469 LOSS : 0.03442874178290367\n",
      "iteration : 316/469 LOSS : 0.04085607826709747\n",
      "iteration : 317/469 LOSS : 0.03681950271129608\n",
      "iteration : 318/469 LOSS : 0.04504530876874924\n",
      "iteration : 319/469 LOSS : 0.04227708280086517\n",
      "iteration : 320/469 LOSS : 0.04006790742278099\n",
      "iteration : 321/469 LOSS : 0.04626953601837158\n",
      "iteration : 322/469 LOSS : 0.03953755646944046\n",
      "iteration : 323/469 LOSS : 0.03837346285581589\n",
      "iteration : 324/469 LOSS : 0.03587542474269867\n",
      "iteration : 325/469 LOSS : 0.03820781782269478\n",
      "iteration : 326/469 LOSS : 0.03941057249903679\n",
      "iteration : 327/469 LOSS : 0.03784487769007683\n",
      "iteration : 328/469 LOSS : 0.03938296064734459\n",
      "iteration : 329/469 LOSS : 0.038272615522146225\n",
      "iteration : 330/469 LOSS : 0.04341684281826019\n",
      "iteration : 331/469 LOSS : 0.040353696793317795\n",
      "iteration : 332/469 LOSS : 0.04227650910615921\n",
      "iteration : 333/469 LOSS : 0.03136372193694115\n",
      "iteration : 334/469 LOSS : 0.03877893462777138\n",
      "iteration : 335/469 LOSS : 0.04115499183535576\n",
      "iteration : 336/469 LOSS : 0.04131123423576355\n",
      "iteration : 337/469 LOSS : 0.032453447580337524\n",
      "iteration : 338/469 LOSS : 0.04152609407901764\n",
      "iteration : 339/469 LOSS : 0.041940297931432724\n",
      "iteration : 340/469 LOSS : 0.041394248604774475\n",
      "iteration : 341/469 LOSS : 0.035135384649038315\n",
      "iteration : 342/469 LOSS : 0.04571036249399185\n",
      "iteration : 343/469 LOSS : 0.03586257994174957\n",
      "iteration : 344/469 LOSS : 0.04528167098760605\n",
      "iteration : 345/469 LOSS : 0.03999900072813034\n",
      "iteration : 346/469 LOSS : 0.03589184582233429\n",
      "iteration : 347/469 LOSS : 0.03761201351881027\n",
      "iteration : 348/469 LOSS : 0.03198598697781563\n",
      "iteration : 349/469 LOSS : 0.03645186498761177\n",
      "iteration : 350/469 LOSS : 0.03230869024991989\n",
      "iteration : 351/469 LOSS : 0.04216497763991356\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration : 352/469 LOSS : 0.03546852245926857\n",
      "iteration : 353/469 LOSS : 0.037138160318136215\n",
      "iteration : 354/469 LOSS : 0.03729572519659996\n",
      "iteration : 355/469 LOSS : 0.04399530217051506\n",
      "iteration : 356/469 LOSS : 0.03311669081449509\n",
      "iteration : 357/469 LOSS : 0.03184093162417412\n",
      "iteration : 358/469 LOSS : 0.03731703385710716\n",
      "iteration : 359/469 LOSS : 0.03377864509820938\n",
      "iteration : 360/469 LOSS : 0.04183118790388107\n",
      "iteration : 361/469 LOSS : 0.04085260629653931\n",
      "iteration : 362/469 LOSS : 0.03883841633796692\n",
      "iteration : 363/469 LOSS : 0.03770909458398819\n",
      "iteration : 364/469 LOSS : 0.04417474567890167\n",
      "iteration : 365/469 LOSS : 0.03995217755436897\n",
      "iteration : 366/469 LOSS : 0.03637806698679924\n",
      "iteration : 367/469 LOSS : 0.03364132344722748\n",
      "iteration : 368/469 LOSS : 0.04077937453985214\n",
      "iteration : 369/469 LOSS : 0.03790133818984032\n",
      "iteration : 370/469 LOSS : 0.033697087317705154\n",
      "iteration : 371/469 LOSS : 0.04278208315372467\n",
      "iteration : 372/469 LOSS : 0.031963322311639786\n",
      "iteration : 373/469 LOSS : 0.030559122562408447\n",
      "iteration : 374/469 LOSS : 0.03810600936412811\n",
      "iteration : 375/469 LOSS : 0.03261606767773628\n",
      "iteration : 376/469 LOSS : 0.04231792688369751\n",
      "iteration : 377/469 LOSS : 0.02853688783943653\n",
      "iteration : 378/469 LOSS : 0.03713085502386093\n",
      "iteration : 379/469 LOSS : 0.03463578596711159\n",
      "iteration : 380/469 LOSS : 0.04263267666101456\n",
      "iteration : 381/469 LOSS : 0.036568209528923035\n",
      "iteration : 382/469 LOSS : 0.03844039514660835\n",
      "iteration : 383/469 LOSS : 0.040726639330387115\n",
      "iteration : 384/469 LOSS : 0.04745171219110489\n",
      "iteration : 385/469 LOSS : 0.04090090095996857\n",
      "iteration : 386/469 LOSS : 0.03693433105945587\n",
      "iteration : 387/469 LOSS : 0.04390091449022293\n",
      "iteration : 388/469 LOSS : 0.029137292876839638\n",
      "iteration : 389/469 LOSS : 0.038529928773641586\n",
      "iteration : 390/469 LOSS : 0.032776784151792526\n",
      "iteration : 391/469 LOSS : 0.042208004742860794\n",
      "iteration : 392/469 LOSS : 0.03045102022588253\n",
      "iteration : 393/469 LOSS : 0.042358335107564926\n",
      "iteration : 394/469 LOSS : 0.041199762374162674\n",
      "iteration : 395/469 LOSS : 0.03948425501585007\n",
      "iteration : 396/469 LOSS : 0.04646492376923561\n",
      "iteration : 397/469 LOSS : 0.03463982790708542\n",
      "iteration : 398/469 LOSS : 0.030865684151649475\n",
      "iteration : 399/469 LOSS : 0.04174087941646576\n",
      "iteration : 400/469 LOSS : 0.04192441329360008\n",
      "iteration : 401/469 LOSS : 0.03169994056224823\n",
      "iteration : 402/469 LOSS : 0.03961788862943649\n",
      "iteration : 403/469 LOSS : 0.033563282340765\n",
      "iteration : 404/469 LOSS : 0.04117072373628616\n",
      "iteration : 405/469 LOSS : 0.039872001856565475\n",
      "iteration : 406/469 LOSS : 0.032322078943252563\n",
      "iteration : 407/469 LOSS : 0.03545437008142471\n",
      "iteration : 408/469 LOSS : 0.03333553671836853\n",
      "iteration : 409/469 LOSS : 0.04604274034500122\n",
      "iteration : 410/469 LOSS : 0.046577226370573044\n",
      "iteration : 411/469 LOSS : 0.04497012495994568\n",
      "iteration : 412/469 LOSS : 0.03804904595017433\n",
      "iteration : 413/469 LOSS : 0.03769047558307648\n",
      "iteration : 414/469 LOSS : 0.04124915972352028\n",
      "iteration : 415/469 LOSS : 0.03723418712615967\n",
      "iteration : 416/469 LOSS : 0.03633056581020355\n",
      "iteration : 417/469 LOSS : 0.03599174693226814\n",
      "iteration : 418/469 LOSS : 0.0346737802028656\n",
      "iteration : 419/469 LOSS : 0.04331051558256149\n",
      "iteration : 420/469 LOSS : 0.03681138902902603\n",
      "iteration : 421/469 LOSS : 0.031042728573083878\n",
      "iteration : 422/469 LOSS : 0.03318052366375923\n",
      "iteration : 423/469 LOSS : 0.038539960980415344\n",
      "iteration : 424/469 LOSS : 0.037799663841724396\n",
      "iteration : 425/469 LOSS : 0.03598899766802788\n",
      "iteration : 426/469 LOSS : 0.03413023054599762\n",
      "iteration : 427/469 LOSS : 0.03676459938287735\n",
      "iteration : 428/469 LOSS : 0.040408823639154434\n",
      "iteration : 429/469 LOSS : 0.03920811042189598\n",
      "iteration : 430/469 LOSS : 0.034479428082704544\n",
      "iteration : 431/469 LOSS : 0.037199150770902634\n",
      "iteration : 432/469 LOSS : 0.03830640763044357\n",
      "iteration : 433/469 LOSS : 0.03263682872056961\n",
      "iteration : 434/469 LOSS : 0.03462616354227066\n",
      "iteration : 435/469 LOSS : 0.04243990778923035\n",
      "iteration : 436/469 LOSS : 0.03534926846623421\n",
      "iteration : 437/469 LOSS : 0.035603929311037064\n",
      "iteration : 438/469 LOSS : 0.03216477856040001\n",
      "iteration : 439/469 LOSS : 0.03813133016228676\n",
      "iteration : 440/469 LOSS : 0.04367217421531677\n",
      "iteration : 441/469 LOSS : 0.03653901070356369\n",
      "iteration : 442/469 LOSS : 0.035215675830841064\n",
      "iteration : 443/469 LOSS : 0.03991975635290146\n",
      "iteration : 444/469 LOSS : 0.029373113065958023\n",
      "iteration : 445/469 LOSS : 0.03382248431444168\n",
      "iteration : 446/469 LOSS : 0.03416145592927933\n",
      "iteration : 447/469 LOSS : 0.04350290447473526\n",
      "iteration : 448/469 LOSS : 0.028168613091111183\n",
      "iteration : 449/469 LOSS : 0.03398493677377701\n",
      "iteration : 450/469 LOSS : 0.03794840723276138\n",
      "iteration : 451/469 LOSS : 0.03215305507183075\n",
      "iteration : 452/469 LOSS : 0.04235316067934036\n",
      "iteration : 453/469 LOSS : 0.03778094798326492\n",
      "iteration : 454/469 LOSS : 0.03786914423108101\n",
      "iteration : 455/469 LOSS : 0.03769552335143089\n",
      "iteration : 456/469 LOSS : 0.03294174373149872\n",
      "iteration : 457/469 LOSS : 0.03213325887918472\n",
      "iteration : 458/469 LOSS : 0.04146803170442581\n",
      "iteration : 459/469 LOSS : 0.033250436186790466\n",
      "iteration : 460/469 LOSS : 0.039718084037303925\n",
      "iteration : 461/469 LOSS : 0.03818054497241974\n",
      "iteration : 462/469 LOSS : 0.03979576751589775\n",
      "iteration : 463/469 LOSS : 0.03004847839474678\n",
      "iteration : 464/469 LOSS : 0.03416388854384422\n",
      "iteration : 465/469 LOSS : 0.035650528967380524\n",
      "iteration : 466/469 LOSS : 0.0363578200340271\n",
      "iteration : 467/469 LOSS : 0.03288043290376663\n",
      "iteration : 468/469 LOSS : 0.03504261001944542\n",
      "EPOCH 1/30, loss = 0.092789, kl_loss = 0.085004, bce_loss = 0.007785\n",
      "Testing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/torch/nn/_reduction.py:43: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: -0.8151, Accuracy: 59155/60000 (99%)\n",
      "\n",
      "iteration : 0/469 LOSS : 0.03657310828566551\n",
      "iteration : 1/469 LOSS : 0.03895779326558113\n",
      "iteration : 2/469 LOSS : 0.03699634596705437\n",
      "iteration : 3/469 LOSS : 0.03460250794887543\n",
      "iteration : 4/469 LOSS : 0.038039810955524445\n",
      "iteration : 5/469 LOSS : 0.031621988862752914\n",
      "iteration : 6/469 LOSS : 0.029758190736174583\n",
      "iteration : 7/469 LOSS : 0.03514045104384422\n",
      "iteration : 8/469 LOSS : 0.03194611147046089\n",
      "iteration : 9/469 LOSS : 0.03142540156841278\n",
      "iteration : 10/469 LOSS : 0.03592536598443985\n",
      "iteration : 11/469 LOSS : 0.030846768990159035\n",
      "iteration : 12/469 LOSS : 0.03437507152557373\n",
      "iteration : 13/469 LOSS : 0.03968574479222298\n",
      "iteration : 14/469 LOSS : 0.03182000294327736\n",
      "iteration : 15/469 LOSS : 0.03133437782526016\n",
      "iteration : 16/469 LOSS : 0.03475688397884369\n",
      "iteration : 17/469 LOSS : 0.038312170654535294\n",
      "iteration : 18/469 LOSS : 0.03478705510497093\n",
      "iteration : 19/469 LOSS : 0.034093089401721954\n",
      "iteration : 20/469 LOSS : 0.03968816250562668\n",
      "iteration : 21/469 LOSS : 0.03468041121959686\n",
      "iteration : 22/469 LOSS : 0.03241787478327751\n",
      "iteration : 23/469 LOSS : 0.03349442407488823\n",
      "iteration : 24/469 LOSS : 0.0313192754983902\n",
      "iteration : 25/469 LOSS : 0.04084977135062218\n",
      "iteration : 26/469 LOSS : 0.027844194322824478\n",
      "iteration : 27/469 LOSS : 0.029865607619285583\n",
      "iteration : 28/469 LOSS : 0.03138486668467522\n",
      "iteration : 29/469 LOSS : 0.030336150899529457\n",
      "iteration : 30/469 LOSS : 0.025780806317925453\n",
      "iteration : 31/469 LOSS : 0.03178029507398605\n",
      "iteration : 32/469 LOSS : 0.026959527283906937\n",
      "iteration : 33/469 LOSS : 0.03197729215025902\n",
      "iteration : 34/469 LOSS : 0.03346819803118706\n",
      "iteration : 35/469 LOSS : 0.027707185596227646\n",
      "iteration : 36/469 LOSS : 0.035784728825092316\n",
      "iteration : 37/469 LOSS : 0.02926712855696678\n",
      "iteration : 38/469 LOSS : 0.03075013868510723\n",
      "iteration : 39/469 LOSS : 0.03378063440322876\n",
      "iteration : 40/469 LOSS : 0.03755643963813782\n",
      "iteration : 41/469 LOSS : 0.032537296414375305\n",
      "iteration : 42/469 LOSS : 0.03298540040850639\n",
      "iteration : 43/469 LOSS : 0.027239307761192322\n",
      "iteration : 44/469 LOSS : 0.03049366921186447\n",
      "iteration : 45/469 LOSS : 0.035692036151885986\n",
      "iteration : 46/469 LOSS : 0.03772171959280968\n",
      "iteration : 47/469 LOSS : 0.031732913106679916\n",
      "iteration : 48/469 LOSS : 0.030262870714068413\n",
      "iteration : 49/469 LOSS : 0.03159377723932266\n",
      "iteration : 50/469 LOSS : 0.028902586549520493\n",
      "iteration : 51/469 LOSS : 0.028863508254289627\n",
      "iteration : 52/469 LOSS : 0.026720676571130753\n",
      "iteration : 53/469 LOSS : 0.03257600963115692\n",
      "iteration : 54/469 LOSS : 0.03569190576672554\n",
      "iteration : 55/469 LOSS : 0.029562348499894142\n",
      "iteration : 56/469 LOSS : 0.033597346395254135\n",
      "iteration : 57/469 LOSS : 0.027929676696658134\n",
      "iteration : 58/469 LOSS : 0.03263513743877411\n",
      "iteration : 59/469 LOSS : 0.03304283693432808\n",
      "iteration : 60/469 LOSS : 0.033197540789842606\n",
      "iteration : 61/469 LOSS : 0.03509443998336792\n",
      "iteration : 62/469 LOSS : 0.02817956916987896\n",
      "iteration : 63/469 LOSS : 0.03511778637766838\n",
      "iteration : 64/469 LOSS : 0.039067503064870834\n",
      "iteration : 65/469 LOSS : 0.029277125373482704\n",
      "iteration : 66/469 LOSS : 0.026862137019634247\n",
      "iteration : 67/469 LOSS : 0.03008977696299553\n",
      "iteration : 68/469 LOSS : 0.03204113617539406\n",
      "iteration : 69/469 LOSS : 0.03179951012134552\n",
      "iteration : 70/469 LOSS : 0.0312468484044075\n",
      "iteration : 71/469 LOSS : 0.03409159183502197\n",
      "iteration : 72/469 LOSS : 0.024213507771492004\n",
      "iteration : 73/469 LOSS : 0.02891271933913231\n",
      "iteration : 74/469 LOSS : 0.03125545755028725\n",
      "iteration : 75/469 LOSS : 0.03377272188663483\n",
      "iteration : 76/469 LOSS : 0.02691543847322464\n",
      "iteration : 77/469 LOSS : 0.029438167810440063\n",
      "iteration : 78/469 LOSS : 0.030904263257980347\n",
      "iteration : 79/469 LOSS : 0.03464856743812561\n",
      "iteration : 80/469 LOSS : 0.029809335246682167\n",
      "iteration : 81/469 LOSS : 0.03455036133527756\n",
      "iteration : 82/469 LOSS : 0.030901741236448288\n",
      "iteration : 83/469 LOSS : 0.03878707066178322\n",
      "iteration : 84/469 LOSS : 0.030347416177392006\n",
      "iteration : 85/469 LOSS : 0.02826152741909027\n",
      "iteration : 86/469 LOSS : 0.02619239315390587\n",
      "iteration : 87/469 LOSS : 0.028388921171426773\n",
      "iteration : 88/469 LOSS : 0.03640520200133324\n",
      "iteration : 89/469 LOSS : 0.03538563847541809\n",
      "iteration : 90/469 LOSS : 0.036260101944208145\n",
      "iteration : 91/469 LOSS : 0.03255587816238403\n",
      "iteration : 92/469 LOSS : 0.0348590686917305\n",
      "iteration : 93/469 LOSS : 0.03573859855532646\n",
      "iteration : 94/469 LOSS : 0.03165818378329277\n",
      "iteration : 95/469 LOSS : 0.03304773196578026\n",
      "iteration : 96/469 LOSS : 0.02935803309082985\n",
      "iteration : 97/469 LOSS : 0.03807968646287918\n",
      "iteration : 98/469 LOSS : 0.030446277931332588\n",
      "iteration : 99/469 LOSS : 0.03015788272023201\n",
      "iteration : 100/469 LOSS : 0.03626897558569908\n",
      "iteration : 101/469 LOSS : 0.024984480813145638\n",
      "iteration : 102/469 LOSS : 0.03327859938144684\n",
      "iteration : 103/469 LOSS : 0.028120726346969604\n",
      "iteration : 104/469 LOSS : 0.028228560462594032\n",
      "iteration : 105/469 LOSS : 0.039494819939136505\n",
      "iteration : 106/469 LOSS : 0.02862141653895378\n",
      "iteration : 107/469 LOSS : 0.028896164149045944\n",
      "iteration : 108/469 LOSS : 0.0312176700681448\n",
      "iteration : 109/469 LOSS : 0.038389310240745544\n",
      "iteration : 110/469 LOSS : 0.026100056245923042\n",
      "iteration : 111/469 LOSS : 0.028244536370038986\n",
      "iteration : 112/469 LOSS : 0.03622545674443245\n",
      "iteration : 113/469 LOSS : 0.02816282957792282\n",
      "iteration : 114/469 LOSS : 0.0288088358938694\n",
      "iteration : 115/469 LOSS : 0.025265060365200043\n",
      "iteration : 116/469 LOSS : 0.027137648314237595\n",
      "iteration : 117/469 LOSS : 0.026708554476499557\n",
      "iteration : 118/469 LOSS : 0.030884871259331703\n",
      "iteration : 119/469 LOSS : 0.025478843599557877\n",
      "iteration : 120/469 LOSS : 0.03422881290316582\n",
      "iteration : 121/469 LOSS : 0.028943173587322235\n",
      "iteration : 122/469 LOSS : 0.028182897716760635\n",
      "iteration : 123/469 LOSS : 0.03417139872908592\n",
      "iteration : 124/469 LOSS : 0.03504740446805954\n",
      "iteration : 125/469 LOSS : 0.024742521345615387\n",
      "iteration : 126/469 LOSS : 0.03399878740310669\n",
      "iteration : 127/469 LOSS : 0.031977955251932144\n",
      "iteration : 128/469 LOSS : 0.03479804843664169\n",
      "iteration : 129/469 LOSS : 0.036027491092681885\n",
      "iteration : 130/469 LOSS : 0.03000842034816742\n",
      "iteration : 131/469 LOSS : 0.029887251555919647\n",
      "iteration : 132/469 LOSS : 0.0282744187861681\n",
      "iteration : 133/469 LOSS : 0.03686245530843735\n",
      "iteration : 134/469 LOSS : 0.030417729169130325\n",
      "iteration : 135/469 LOSS : 0.029458049684762955\n",
      "iteration : 136/469 LOSS : 0.03702014684677124\n",
      "iteration : 137/469 LOSS : 0.031030766665935516\n",
      "iteration : 138/469 LOSS : 0.026837320998311043\n",
      "iteration : 139/469 LOSS : 0.027700237929821014\n",
      "iteration : 140/469 LOSS : 0.03315301239490509\n",
      "iteration : 141/469 LOSS : 0.024747714400291443\n",
      "iteration : 142/469 LOSS : 0.030467092990875244\n",
      "iteration : 143/469 LOSS : 0.03565090522170067\n",
      "iteration : 144/469 LOSS : 0.030422504991292953\n",
      "iteration : 145/469 LOSS : 0.03412972018122673\n",
      "iteration : 146/469 LOSS : 0.03268364816904068\n",
      "iteration : 147/469 LOSS : 0.026777366176247597\n",
      "iteration : 148/469 LOSS : 0.028406953439116478\n",
      "iteration : 149/469 LOSS : 0.02269427850842476\n",
      "iteration : 150/469 LOSS : 0.029433604329824448\n",
      "iteration : 151/469 LOSS : 0.0302464310079813\n",
      "iteration : 152/469 LOSS : 0.03709027171134949\n",
      "iteration : 153/469 LOSS : 0.03010774776339531\n",
      "iteration : 154/469 LOSS : 0.027409369125962257\n",
      "iteration : 155/469 LOSS : 0.02610943093895912\n",
      "iteration : 156/469 LOSS : 0.033166125416755676\n",
      "iteration : 157/469 LOSS : 0.03403962403535843\n",
      "iteration : 158/469 LOSS : 0.02766473963856697\n",
      "iteration : 159/469 LOSS : 0.027056975290179253\n",
      "iteration : 160/469 LOSS : 0.027930913493037224\n",
      "iteration : 161/469 LOSS : 0.02691437304019928\n",
      "iteration : 162/469 LOSS : 0.02968856506049633\n",
      "iteration : 163/469 LOSS : 0.03002907522022724\n",
      "iteration : 164/469 LOSS : 0.031242553144693375\n",
      "iteration : 165/469 LOSS : 0.03331051766872406\n",
      "iteration : 166/469 LOSS : 0.029067976400256157\n",
      "iteration : 167/469 LOSS : 0.024308374151587486\n",
      "iteration : 168/469 LOSS : 0.02985917590558529\n",
      "iteration : 169/469 LOSS : 0.030898442491889\n",
      "iteration : 170/469 LOSS : 0.030491478741168976\n",
      "iteration : 171/469 LOSS : 0.03294173628091812\n",
      "iteration : 172/469 LOSS : 0.028434937819838524\n",
      "iteration : 173/469 LOSS : 0.029749371111392975\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration : 174/469 LOSS : 0.030787063762545586\n",
      "iteration : 175/469 LOSS : 0.028991825878620148\n",
      "iteration : 176/469 LOSS : 0.02686242014169693\n",
      "iteration : 177/469 LOSS : 0.029221685603260994\n",
      "iteration : 178/469 LOSS : 0.030162792652845383\n",
      "iteration : 179/469 LOSS : 0.037090983241796494\n",
      "iteration : 180/469 LOSS : 0.026116076856851578\n",
      "iteration : 181/469 LOSS : 0.033521056175231934\n",
      "iteration : 182/469 LOSS : 0.03283488005399704\n",
      "iteration : 183/469 LOSS : 0.03086806833744049\n",
      "iteration : 184/469 LOSS : 0.028422340750694275\n",
      "iteration : 185/469 LOSS : 0.027291283011436462\n",
      "iteration : 186/469 LOSS : 0.03337002918124199\n",
      "iteration : 187/469 LOSS : 0.02644030563533306\n",
      "iteration : 188/469 LOSS : 0.032923661172389984\n",
      "iteration : 189/469 LOSS : 0.03176918625831604\n",
      "iteration : 190/469 LOSS : 0.027662312611937523\n",
      "iteration : 191/469 LOSS : 0.0306413471698761\n",
      "iteration : 192/469 LOSS : 0.028507230803370476\n",
      "iteration : 193/469 LOSS : 0.02474455162882805\n",
      "iteration : 194/469 LOSS : 0.03587568551301956\n",
      "iteration : 195/469 LOSS : 0.023057958111166954\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/Cellar/python/3.7.5/Frameworks/Python.framework/Versions/3.7/lib/python3.7/multiprocessing/queues.py\", line 242, in _feed\n",
      "    send_bytes(obj)\n",
      "  File \"/usr/local/Cellar/python/3.7.5/Frameworks/Python.framework/Versions/3.7/lib/python3.7/multiprocessing/connection.py\", line 200, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "  File \"/usr/local/Cellar/python/3.7.5/Frameworks/Python.framework/Versions/3.7/lib/python3.7/multiprocessing/connection.py\", line 404, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/usr/local/Cellar/python/3.7.5/Frameworks/Python.framework/Versions/3.7/lib/python3.7/multiprocessing/connection.py\", line 368, in _send\n",
      "    n = write(self._handle, buf)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n",
      "  File \"/usr/local/Cellar/python/3.7.5/Frameworks/Python.framework/Versions/3.7/lib/python3.7/multiprocessing/queues.py\", line 242, in _feed\n",
      "    send_bytes(obj)\n",
      "  File \"/usr/local/Cellar/python/3.7.5/Frameworks/Python.framework/Versions/3.7/lib/python3.7/multiprocessing/connection.py\", line 200, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "  File \"/usr/local/Cellar/python/3.7.5/Frameworks/Python.framework/Versions/3.7/lib/python3.7/multiprocessing/connection.py\", line 404, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/usr/local/Cellar/python/3.7.5/Frameworks/Python.framework/Versions/3.7/lib/python3.7/multiprocessing/connection.py\", line 368, in _send\n",
      "    n = write(self._handle, buf)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/Cellar/python/3.7.5/Frameworks/Python.framework/Versions/3.7/lib/python3.7/multiprocessing/queues.py\", line 242, in _feed\n",
      "    send_bytes(obj)\n",
      "  File \"/usr/local/Cellar/python/3.7.5/Frameworks/Python.framework/Versions/3.7/lib/python3.7/multiprocessing/connection.py\", line 200, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "  File \"/usr/local/Cellar/python/3.7.5/Frameworks/Python.framework/Versions/3.7/lib/python3.7/multiprocessing/connection.py\", line 404, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/usr/local/Cellar/python/3.7.5/Frameworks/Python.framework/Versions/3.7/lib/python3.7/multiprocessing/connection.py\", line 368, in _send\n",
      "    n = write(self._handle, buf)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-c010a3c68b4e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequires_grad_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m         \u001b[0mtrain_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    164\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m         \"\"\"\n\u001b[0;32m--> 166\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     98\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def eval():\n",
    "  model.eval()\n",
    "  test_loss = 0\n",
    "  correct = 0\n",
    "  with torch.no_grad():\n",
    "    for data, target in test_loader:\n",
    "      z, output = model(data)\n",
    "      test_loss += F.nll_loss(output, target, size_average=False).item()\n",
    "      pred = output.data.max(1, keepdim=True)[1]\n",
    "      correct += pred.eq(target.data.view_as(pred)).sum()\n",
    "  test_loss /= len(test_loader.dataset)\n",
    "#   test_losses.append(test_loss)\n",
    "  print('\\nTest set: Avg. loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "    test_loss, correct, len(test_loader.dataset),\n",
    "    100. * correct / len(test_loader.dataset)))\n",
    "\n",
    "\n",
    "\n",
    "epochs = 30\n",
    "l_gradient_penalty = 0.05\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    loss = 0\n",
    "    bce = 0\n",
    "    GP = 0\n",
    "    for i, batch in enumerate(train_loader):\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        x, y = batch\n",
    "        y = F.one_hot(y, num_classes=10).float()\n",
    "\n",
    "        x, y = x.to(device), y.to(device)\n",
    "\n",
    "        x.requires_grad_(True)\n",
    "\n",
    "        z, y_pred = model(x)\n",
    "        bce_loss = F.binary_cross_entropy(y_pred, y)\n",
    "        GP_loss = l_gradient_penalty * calc_gradient_penalty(x, y_pred.sum(1))\n",
    "        train_loss = bce_loss + GP_loss\n",
    "        x.requires_grad_(False)\n",
    "\n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            model.update_embeddings(x, y)   \n",
    "        loss += train_loss.item()\n",
    "        bce += bce_loss.item()\n",
    "        GP += GP_loss.item()\n",
    "        print(\"iteration : {}/{} LOSS : {}\".format(i, len(train_loader), train_loss.item()))\n",
    "    loss = loss/len(train_loader)\n",
    "    bce = bce/len(train_loader)\n",
    "    GP = GP/len(train_loader)\n",
    "            \n",
    "    print(\"EPOCH {}/{}, loss = {:.6f}, kl_loss = {:.6f}, bce_loss = {:.6f}\".format(epoch+1, epochs, loss, bce, GP))\n",
    "    print('Testing...')\n",
    "    eval()\n",
    "    torch.save(model.state_dict(), './duq_mnist_{}.pth'.format(epoch+1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('./duq_mnist_1.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000\n",
      "tensor(9)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.5483e-03, 6.3488e-04, 5.3132e-04, 2.0260e-03, 4.4799e-02, 4.3634e-03,\n",
      "         6.9272e-05, 3.9551e-02, 1.1885e-02, 8.2298e-01]],\n",
      "       grad_fn=<ExpBackward>)\n"
     ]
    }
   ],
   "source": [
    "dataloader_iterator = iter(test_loader)\n",
    "print(len(test_loader.dataset))\n",
    "x, y = next(dataloader_iterator)\n",
    "i = 4\n",
    "# y = F.one_hot(y, num_classes=10).float()\n",
    "print(y[i])\n",
    "img = x[i].view(28, 28).data\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(img, cmap='gray')\n",
    "plt.show()  \n",
    "\n",
    "z, y_pred = model(x[i].unsqueeze(0))\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "  model.eval()\n",
    "  test_loss = 0\n",
    "  correct = 0\n",
    "  with torch.no_grad():\n",
    "    for data, target in test_loader:\n",
    "      z, output = model(data)\n",
    "      test_loss += 0 #F.nll_loss(output, target, size_average=False).item()\n",
    "      pred = output.data.max(1, keepdim=True)[1]\n",
    "      correct += pred.eq(target.data.view_as(pred)).sum()\n",
    "  test_loss /= len(test_loader.dataset)\n",
    "#   test_losses.append(test_loss)\n",
    "  print('\\nTest set: Avg. loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "    test_loss, correct, len(test_loader.dataset),\n",
    "    100. * correct / len(test_loader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000\n",
      "tensor(9)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[9.7070e-04, 2.4392e-04, 4.1119e-04, 1.5993e-03, 2.8569e-02, 2.1421e-03,\n",
      "         3.5625e-05, 2.4914e-02, 1.1930e-02, 9.0309e-01]],\n",
      "       grad_fn=<ExpBackward>)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0044, 0.0036, 0.0029, 0.0481, 0.0248, 0.5078, 0.0315, 0.0016, 0.0805,\n",
      "         0.0250]], grad_fn=<ExpBackward>)\n"
     ]
    }
   ],
   "source": [
    "dataloader_iterator = iter(test_loader)\n",
    "print(len(test_loader.dataset))\n",
    "x, y = next(dataloader_iterator)\n",
    "i = 4\n",
    "print(y[i])\n",
    "img = x[i].view(28, 28).data\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(img, cmap='gray')\n",
    "plt.show()  \n",
    "\n",
    "z, y_pred = model(x[i].unsqueeze(0))\n",
    "print(y_pred)\n",
    "\n",
    "def rotate_img(x, deg):\n",
    "    import scipy.ndimage as nd\n",
    "    return nd.rotate(x.reshape(28,28),deg,reshape=False).ravel()\n",
    "\n",
    "rotated = rotate_img(img, 120).reshape(28, 28)\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(rotated, cmap='gray')\n",
    "plt.show()  \n",
    "\n",
    "z, y_pred = model(torch.from_numpy(rotated.reshape(1, 1, 28, 28)))\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN_DUQ(\n",
      "  (conv1): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv2): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (bn3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc1): Linear(in_features=512, out_features=256, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
